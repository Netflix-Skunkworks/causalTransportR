[{"path":"/articles/generalization.html","id":"naive","dir":"Articles","previous_headings":"","what":"naive","title":"Effect Generalization","text":"Using AIPW estimator naively complete data ignoring missingness problem, gives us wrong answer (know true effect 1.1739339).","code":"cat(\"\\n Naive \\n\") ##  ##  Naive naive_fit <- ateGT(y[s == 1], a[s == 1], X[s == 1, ],         nuisMod = \"rlm\", target = \"insample\", noi = F ) naive_fit %>% summary() ##      parameter       est         se     ci.ll     ci.ul pval ## 1      E{Y(0)} 0.3518456 0.02111955 0.3104513 0.3932399    0 ## 2      E{Y(1)} 1.2192154 0.02207557 1.1759473 1.2624836    0 ## 3 E{Y(1)-Y(0)} 0.8673699 0.02864512 0.8112254 0.9235143    0"},{"path":"/articles/generalization.html","id":"corrected-estimators","dir":"Articles","previous_headings":"","what":"corrected estimators","title":"Effect Generalization","text":"Next demonstrate use estimators implemented ateGT simulated data .","code":""},{"path":"/articles/generalization.html","id":"aisw","dir":"Articles","previous_headings":"corrected estimators","what":"AISW","title":"Effect Generalization","text":"reweighting estimator, good idea study balance. plot method ateGT function produces following plot, suggests substantial improvements balance thanks selection weights.  better naive approach cases.","code":"cat(\"\\nAISW \\n\") ##  ## AISW aipwfit <- ateGT(y, a, X, s,         treatProb = treatprob, nuisMod = \"rlm\",         estimator = \"AISW\", target = \"generalize\", noi = F ) aipwfit %>% summary ##      parameter       est         se     ci.ll     ci.ul pval ## 1      E{Y(0)} 0.2790733 0.03104175 0.2182314 0.3399151    0 ## 2      E{Y(1)} 1.5089650 0.02994849 1.4502659 1.5676640    0 ## 3 E{Y(1)-Y(0)} 1.2298917 0.04250149 1.1465888 1.3131946    0 aipwfit %>% plot(X) cat(\"\\nISW \\n\") ##  ## ISW ipwfit <- ateGT(y, a, X, s,         treatProb = treatprob, nuisMod = \"rlm\",         estimator = \"ISW\", target = \"generalize\", noi = F ) ## Warning in ateGT(y, a, X, s, treatProb = treatprob, nuisMod = \"rlm\", estimator = \"ISW\", : Analytic SEs may be inacurrate; use the bootstrap ipwfit %>% summary ##      parameter       est         se     ci.ll     ci.ul pval ## 1      E{Y(0)} 0.2816738 0.03347853 0.2160558 0.3472917    0 ## 2      E{Y(1)} 1.4553181 0.06064194 1.3364599 1.5741763    0 ## 3 E{Y(1)-Y(0)} 1.1736443 0.06985879 1.0367211 1.3105675    0 cat(\"\\nOM \\n\") ##  ## OM omfit <- ateGT(y, a, X, s,         treatProb = treatprob, nuisMod = \"rlm\",         estimator = \"OM\", target = \"generalize\", noi = F ) ## Warning in ateGT(y, a, X, s, treatProb = treatprob, nuisMod = \"rlm\", estimator = \"OM\", : Analytic SEs may be inacurrate; use the bootstrap omfit %>% summary ##      parameter       est          se     ci.ll     ci.ul pval ## 1      E{Y(0)} 0.3034576 0.005016487 0.2936253 0.3132899    0 ## 2      E{Y(1)} 1.4439125 0.007368723 1.4294698 1.4583552    0 ## 3 E{Y(1)-Y(0)} 1.1404549 0.005082201 1.1304938 1.1504160    0 cat(\"\\nACW \\n\") ##  ## ACW acwfit <- ateGT(y, a, polySieveM(X), s,         treatProb = treatprob, nuisMod = \"rlm\",         estimator = \"ACW\", target = \"generalize\", noi = F ) ## Warning in ateGT(y, a, polySieveM(X), s, treatProb = treatprob, nuisMod = \"rlm\", : Analytic SEs may be inacurrate; use the bootstrap acwfit %>% summary ##      parameter      est         se     ci.ll     ci.ul pval ## 1      E{Y(0)} 0.293218 0.02009422 0.2538333 0.3326026    0 ## 2      E{Y(1)} 1.490452 0.01966189 1.4519143 1.5289889    0 ## 3 E{Y(1)-Y(0)} 1.197234 0.02699272 1.1443279 1.2501394    0"},{"path":"/articles/mlrate.html","id":"naive-difference-in-means","dir":"Articles","previous_headings":"","what":"naive difference in means","title":"Covariate Adjustment using ML predictions","text":"","code":"library(estimatr) # for linear regression with robust SEs effect_se <- function(x) {     x %>%         summary() %>%         .$coefficients %>%         .[2, 1:2] }  df <- dgp() ## true effect  ## 1.187692 lm_robust(df$y ~ df$a) %>% effect_se() ##   Estimate Std. Error  ## 1.35815280 0.09155451"},{"path":"/articles/mlrate.html","id":"linear-covariate-adjustment","dir":"Articles","previous_headings":"","what":"linear covariate adjustment","title":"Covariate Adjustment using ML predictions","text":"","code":"lm_robust(df$y ~ df$a + df$X) %>% effect_se() ##   Estimate Std. Error  ## 1.28067537 0.06761985"},{"path":"/articles/mlrate.html","id":"ml-covariate-adjustment","dir":"Articles","previous_headings":"","what":"ML covariate adjustment","title":"Covariate Adjustment using ML predictions","text":"Adjust linear interaction terms LASSO. References Guo, Y., D. Coey, M. Konutgan, W. Li, C. Schoener, M. Goldman. (2021): “Machine Learning Variance Reduction Online Experiments,” arXiv [stat.ML]","code":"mlRate(df$y, df$a, polySieveM(df$X, k = 2, m = 2), nuisMod = \"rlm\") ##          a          g  ## 1.27166289 0.06816104"},{"path":"/articles/omnibus.html","id":"simulation-with-heterogeneity","dir":"Articles","previous_headings":"","what":"simulation with heterogeneity","title":"Omnibus Tests for Treatment Effect Heterogeneity","text":"p-value \\(\\approx 0\\) - conclude systematic heterogeneity.","code":"with(dgp(), dfmTest(y, a, X)) ## [1]  8.543073e+02 4.322510e-177"},{"path":"/articles/omnibus.html","id":"simulation-without-heterogeneity","dir":"Articles","previous_headings":"","what":"simulation without heterogeneity","title":"Omnibus Tests for Treatment Effect Heterogeneity","text":"large p-value - conclude systematic heterogeneity. References Ding, P., . Feller, L. Miratrix. (2019): “Decomposing Treatment Effect Variation,” Journal American Statistical Association, 114, 304–17.","code":"with(dgp(tauF = function(x) 1/3), dfmTest(y, a, X)) ## [1] 17.52836639  0.06346002"},{"path":[]},{"path":"/articles/tech_details.html","id":"data-preliminaries","dir":"Articles","previous_headings":"Setup","what":"Data Preliminaries","title":"Technical Details : identification and estimation","text":"\\(\\\\Sett{} := \\SetB{1, \\dots, K}\\) treatment \\(X \\\\R^p\\) covariates \\(Z \\\\R^q\\) surrogate (short-term) outcomes \\(Y \\\\R\\) outcome \\(S \\\\SetB{0, 1}\\) selection indicator: takes value 1 experimental sample, 0 otherwise. conjecture existence potential outcomes \\(Y^\\) \\(Z^\\) value treatment \\(\\) long-term surrogate outcomes respectively. selection indicator \\(S\\) defines two subpopulations. \\(S = 1\\) : trial sample. observe \\(\\SetB{A_i, X_i, Y_i}_{= 1}^n\\) observations. Call individuals \\(\\Sett{S}_1\\). Individual covariates \\(X_i\\) observed observation trial sample. Call \\(\\Sett{S}_0\\). treatment \\(\\) may may observed, outcome \\(Y\\) never observed. Aggregate summary statistics : \\(\\Ol{X}\\) sample averages higher moments observed extrapolation sample. treatment outcome information available. union subpopulations called overall sample \\(\\Sett{S}_0 \\cup \\Sett{S}_1\\). data, estimators rely handful nuisance functions (sample analogues \\(\\wh{\\alpha}\\) fit using machine learning estimators cross-fitting) \\(\\mu^(x) = \\Exp{Y | = , X = x}\\): Outcome model. Typically estimable \\(\\Sett{S}_1\\). \\(\\pi^(x) = \\Prob{= | X = x , S = 1}\\): Treatment propensity model (henceforth Propensity score). Typically known RCT (.e. \\(\\Sett{S}_1\\) sample). \\(\\rho(x) = \\Prob{S = 1 | X = x}\\) : Selection propensity model (henceforth Selection score). Typically unknown. \\(\\nu(, z, x) = \\Exp{Y | = , Z = z, X = x}\\): Augmented outcome model includes surrogate predictors.","code":""},{"path":"/articles/tech_details.html","id":"estimands","dir":"Articles","previous_headings":"Setup","what":"Estimands","title":"Technical Details : identification and estimation","text":"may interested estimands. write marginal means \\(\\phi\\) causal contrasts \\(\\tau\\).","code":""},{"path":"/articles/tech_details.html","id":"baseline","dir":"Articles","previous_headings":"Setup > Estimands","what":"Baseline","title":"Technical Details : identification and estimation","text":"\\[ \\phi^1_a = \\Exp{Y^| S = 1} \\] \\[ \\tau^1(, ') = \\Exp{\\tau^1_{, '}(X) | S = 1} = \\Exp{Y^- Y^{'} | S = 1} \\] identified standard assumptions since treatment randomized experimental sample.","code":""},{"path":"/articles/tech_details.html","id":"generalisation","dir":"Articles","previous_headings":"Setup > Estimands","what":"Generalisation","title":"Technical Details : identification and estimation","text":"\\[ \\phi_a = \\Exp{Y^} \\] \\[ \\tau(, ') = \\Exp{ \\tau^0_{, '}(X)} = \\Exp{Y^- Y^{'} } \\] treatment effect overall population \\(\\Sett{S} := \\Sett{S}_1 \\cup \\Sett{S}_0\\), encompasses trial sample extrapolation sample.","code":""},{"path":"/articles/tech_details.html","id":"transportation","dir":"Articles","previous_headings":"Setup > Estimands","what":"Transportation","title":"Technical Details : identification and estimation","text":"\\[ \\phi^0_a = \\Exp{Y^| S = 0} \\] \\[ \\tau^0(, ') = \\Exp{\\tau^0_{, '}(X) | S = 0} = \\Exp{Y^- Y^{'} | S = 0} \\] Estimating challenging don’t observed \\(Y\\) anyone \\(\\Sett{S}_0\\) sample, must extrapolate estimates \\(\\tau(\\cdot)\\) \\(\\Sett{S}_1\\) sample. potential outcomes missing.","code":""},{"path":"/articles/tech_details.html","id":"identification-assumptions","dir":"Articles","previous_headings":"Setup","what":"Identification Assumptions","title":"Technical Details : identification and estimation","text":"Consistency / SUTVA : \\(Y_i = \\Indic(A_i = ) Y^a_i\\). rules interference. Unconfoundedness experiment: \\(Y^0, \\dots, Y^\\indep | X = x, S = 1\\). guaranteed randomization. Propensity overlap trial: \\(\\Prob{= | X = x , S = 1}\\) Selection overlap: \\(\\Prob{S = 1 | X = x) > 0}\\) \\(Y^0, \\dots, Y^\\indep S | X = x\\). requires trial participation good random given covariates \\(X\\). Note weaker Missingness Completely Random (MCAR), require unconditional independence potential outcomes selection obviate need adjustment. can weakened Marginal model / Heterogeneous effect stability: requires ‘outcome model stability’ (\\(\\phi_a(X | S = 0) = \\phi_a(X | S = 1)\\)), .e. expected potential outcomes function covariates identical across trial extrapolation populations. also implies causal contrasts stable across trial extrapolation populations. \\(\\Exp{\\tau_{, '}(X)| S = 1} = \\Exp{\\tau_{, '}(X)| S = 0}\\) Mean exchangeability: \\(\\Exp{Y^| X = x, S = 1, = } = \\Exp{Y^| X = x, = } = \\Exp{Y^| X = x}\\). A1-A4, marginal means \\(\\phi_a, \\phi^0_a\\), causal contrasts \\(\\tau(, ')\\) \\(\\tau^0(, ')\\) overall target sample respectively point identified. A4 demanding condition must evaluated care use case. weak violations A4, partial identification still possible.","code":""},{"path":"/articles/tech_details.html","id":"estimators","dir":"Articles","previous_headings":"","what":"Estimators","title":"Technical Details : identification and estimation","text":"write estimators generic counterfactual means \\(\\phi\\). estimator characterised influence function \\(\\psi(z)\\), solves moment condition \\(\\Exp{\\psi(\\cdot)} = 0\\), \\(\\psi(z)\\) typically form \\(\\text{Estimator} - \\tau\\) obeys \\[ \\sqrt{n} (\\wh{\\phi} - \\phi) = \\frac{1}{n} \\sumin \\psi(z) / \\sqrt{n} + O_p(1) \\] makes Regular Asymptotically Normal (RAL) allows us construct valid confidence intervals. influence function, omit \\(-\\tau\\) piece clarity. illustrate estimation library, generate synthetic experimental data illustrate use ateGT function use case. true generalization effect 1.1671402. true transportation effect 1.585578. different selection score includes \\(X_3\\), also moderates treatment effect. naive difference means severely downward biased. Using doubly-robust AIPW estimator improve things much.","code":"# simulate RCT treatprob <- 0.5 # workhorse dgp <- function(n = 10000, p = 10, treat.prob = treatprob,                 # bounds of X                 Xbounds = c(-1, 1),                 # nonlinear heterogeneity                 tauF = function(x) 1 / exp(-x[3]),                 # nonlinear y0                 y0F = function(x) pmax(x[1] + x[2], 0) + sin(x[5]) * pmax(x[7], 0.5),                 # nonlinear selection                 selF = function(x) x[1] - 3 * x[3] + pmax(x[4], 0)) {     X <<- matrix(runif(n * p, Xbounds[1], Xbounds[2]), n, p)     a <<- rbinom(n, 1, treat.prob)     # generate outcomes using supplied functions     TAU <<- apply(X, 1, tauF)     Y0 <- apply(X, 1, y0F)     selscore <- apply(X, 1, selF)     # outcome     y <<- (a * TAU + Y0 + rnorm(n))     # selection     s <<- rbinom(n, 1, plogis(selscore)) |> as.logical()     # set outcomes for s = 0 as missing     y[s == 0] <<- NA     cat(\"True effect\\n\")     cat(mean(TAU)) } # this function populates the global namespace using <<- ; not recommended for real use dgp() ## True effect ## 1.16714 mean(y[s == 1 & a == 1]) - mean(y[s == 1 & a == 0]) ## [1] 0.8135869 aipw(y[s == 1], a[s == 1], X[s == 1, ], noi = F)$res ##      parameter       est         se     ci.ll     ci.ul pval ## 1      E{Y(0)} 0.3630470 0.02128147 0.3213353 0.4047587    0 ## 2      E{Y(1)} 1.1918666 0.02194327 1.1488578 1.2348754    0 ## 3 E{Y(1)-Y(0)} 0.8288196 0.02867180 0.7726228 0.8850163    0"},{"path":"/articles/tech_details.html","id":"outcome-modelling-om","dir":"Articles","previous_headings":"Estimators","what":"Outcome Modelling (OM)","title":"Technical Details : identification and estimation","text":"high quality estimation conditional response surfaces \\(\\Exp{Y | = , S = 1}\\) feasible, outcome model truly stable across treatment selection subsamples (A4b), counterfactual means causal contrasts generalization transportation cases identified simple extrapolation relevant samples.","code":""},{"path":"/articles/tech_details.html","id":"generalisation-1","dir":"Articles","previous_headings":"Estimators > Outcome Modelling (OM)","what":"generalisation","title":"Technical Details : identification and estimation","text":"\\[ \\Exp{Y^} = \\Expt{\\Sett{S}_0 \\cup \\Sett{S}_1}{\\mu^(X)} \\] expectation taken respect overall sample (.e. marginalizing \\(S=0\\) \\(S=1\\) subsamples). sample analogue \\[ \\wh{\\phi}_a = \\frac{1}{\\abs{\\Sett{S}_0 \\cup \\Sett{S}_1}} \\sum_{\\\\Sett{S}_0 \\cup \\Sett{S}_1} \\wh{\\mu}^{}(X_i) \\]","code":"ateGT(y, a, X, s, target = \"generalize\", estimator = \"OM\", noi = F)$res ##      parameter       est          se     ci.ll     ci.ul pval ## 1      E{Y(0)} 0.3156708 0.005133740 0.3056087 0.3257329    0 ## 2      E{Y(1)} 1.4340213 0.007440637 1.4194376 1.4486049    0 ## 3 E{Y(1)-Y(0)} 1.1183505 0.005334528 1.1078948 1.1288062    0"},{"path":"/articles/tech_details.html","id":"transportation-1","dir":"Articles","previous_headings":"Estimators > Outcome Modelling (OM)","what":"transportation","title":"Technical Details : identification and estimation","text":"\\[ \\Exp{Y^} = \\Expt{\\Sett{S}_0}{\\mu^(X)} \\] sample analogue \\[ \\wh{\\phi}^0_a = \\frac{1}{\\abs{\\Sett{S}_0}} \\sum_{\\\\Sett{S}_0} \\wh{\\mu}^{}(X_i) \\]","code":"ateGT(y, a, X, s, target = \"transport\", estimator = \"OM\", noi = F)$res ##      parameter       est          se     ci.ll     ci.ul pval ## 1      E{Y(0)} 0.2600017 0.007354422 0.2455871 0.2744164    0 ## 2      E{Y(1)} 1.7208079 0.009964147 1.7012782 1.7403376    0 ## 3 E{Y(1)-Y(0)} 1.4608062 0.006205822 1.4486428 1.4729696    0"},{"path":"/articles/tech_details.html","id":"reweighting","dir":"Articles","previous_headings":"Estimators","what":"Reweighting","title":"Technical Details : identification and estimation","text":"Another reasonable strategy reweight observations \\(\\Sett{S}_1\\) resemble target sample (either overall sample \\(\\Sett{S}_0 \\cup \\Sett{S}_1\\) extrapolation sample \\(\\Sett{S}_0\\)). Different target samples imply different weights. Generic estimators two targets \\[ \\Exp{Y^} = \\omega_i  \\frac{\\Indic{= }}{\\pi^(X)} Y \\] \\[ \\Exp{Y^| S = 0} = \\omega_i^0  \\frac{\\Indic{= }}{\\pi^(X)} Y \\]","code":""},{"path":"/articles/tech_details.html","id":"inverse-selection-weights-isw","dir":"Articles","previous_headings":"Estimators > Reweighting","what":"Inverse Selection Weights (ISW)","title":"Technical Details : identification and estimation","text":"Inverse selection scores obvious candidate first set weights.","code":""},{"path":"/articles/tech_details.html","id":"generalisation-2","dir":"Articles","previous_headings":"Estimators > Reweighting > Inverse Selection Weights (ISW)","what":"generalisation","title":"Technical Details : identification and estimation","text":"\\[ \\Exp{Y^} = \\frac{S}{\\rho(X)}  \\frac{\\Indic{= }}{\\pi^(X)} Y \\] inverse selection weights \\(1/\\rho(X)\\) downweight observations -represented experimental sample relative overall sample, vice versa. Sample analogue: \\[ \\wh{\\phi}_a = \\sum_{\\\\Sett{S}_0 \\cup \\Sett{S}_1} \\frac{S_i}{\\wh{\\rho}(X_i)}  \\frac{\\Indic{= }}{\\wh{\\pi}^(X_i)} Y_i \\]","code":"ateGT(y, a, X, s, target = \"generalize\", estimator = \"ISW\", noi = F)$res ##      parameter       est         se     ci.ll     ci.ul pval ## 1      E{Y(0)} 0.3253488 0.03124566 0.2641073 0.3865903    0 ## 2      E{Y(1)} 1.4652592 0.06120733 1.3452928 1.5852256    0 ## 3 E{Y(1)-Y(0)} 1.1399104 0.06941169 1.0038634 1.2759573    0"},{"path":"/articles/tech_details.html","id":"transportation-2","dir":"Articles","previous_headings":"Estimators > Reweighting > Inverse Selection Weights (ISW)","what":"transportation","title":"Technical Details : identification and estimation","text":"\\[ \\Exp{Y^| S = 0} = \\frac{S(1 - \\rho(X))}{\\rho(X)}  \\frac{\\Indic{= }}{\\pi^(X)} Y \\] inverse selection weights \\((1-\\rho(X))/\\rho(X)\\) downweight observations -represented experimental sample relative extrapolation sample, vice versa. Sample analogue: \\[ \\wh{\\phi}^1_a = \\frac{1}{\\abs{\\Sett{S}_0}} \\sum_{\\\\Sett{S}_1} \\frac{S_i(1 - \\wh{\\rho}(X_i))}{\\wh{\\rho}(X_i)}  \\frac{\\Indic{= }}{\\wh{\\pi}^(X_i)} Y_i \\]","code":"ateGT(y, a, X, s, target = \"transport\", estimator = \"ISW\", noi = F)$res ##      parameter       est         se     ci.ll     ci.ul pval ## 1      E{Y(0)} 0.2738876 0.03650136 0.2023449 0.3454303    0 ## 2      E{Y(1)} 1.8460260 0.08032468 1.6885897 2.0034624    0 ## 3 E{Y(1)-Y(0)} 1.5721384 0.08849108 1.3986959 1.7455810    0"},{"path":"/articles/tech_details.html","id":"augmented-isw-aisw","dir":"Articles","previous_headings":"Estimators > Reweighting","what":"Augmented ISW (AISW)","title":"Technical Details : identification and estimation","text":"may want combine ISW OM approaches robust approach works well either \\(\\rho, \\pi\\) \\(\\mu\\) correctly specified","code":""},{"path":"/articles/tech_details.html","id":"generalisation-3","dir":"Articles","previous_headings":"Estimators > Reweighting > Augmented ISW (AISW)","what":"generalisation","title":"Technical Details : identification and estimation","text":"\\[ \\Exp{Y^} = \\mu(, X)  +       \\frac{1}{\\rho(X)}       \\frac{\\Indic{= }}{\\pi^(X)} (Y - \\mu^(X) ) \\] estimator ‘augments’ outcome model weighted average residuals (\\(Y - \\mu(\\cdot)\\)). Sample analogue: \\[ \\wh{\\phi}_a = \\frac{1}{\\abs{\\Sett{S}_0 \\cup \\Sett{S}_1}} \\sum_{\\\\Sett{S}_0 \\cup \\Sett{S}_1} \\wh{\\mu}^{}(X_i) +   \\frac{S_i}{\\wh{\\rho}(X_i)}       \\frac{\\Indic{= }}{\\wh{\\pi}^(X_i)} (Y_i - \\wh{\\mu}^(X_i) ) \\]","code":"ateGT(y, a, X, s, target = \"generalize\", estimator = \"AISW\", noi = F)$res ##      parameter       est         se     ci.ll     ci.ul pval ## 1      E{Y(0)} 0.3026286 0.02816369 0.2474278 0.3578295    0 ## 2      E{Y(1)} 1.4963845 0.02742900 1.4426237 1.5501454    0 ## 3 E{Y(1)-Y(0)} 1.1937559 0.03866555 1.1179714 1.2695404    0"},{"path":"/articles/tech_details.html","id":"transportation-3","dir":"Articles","previous_headings":"Estimators > Reweighting > Augmented ISW (AISW)","what":"transportation","title":"Technical Details : identification and estimation","text":"\\[ \\Exp{Y^| S = 0} = \\mu(, X)  +       \\frac{1}{\\rho(X)}       \\frac{\\Indic{= }}{\\pi^(X)} (Y - \\mu^(X) ) \\] Sample analogue: \\[ \\wh{\\phi}^1_a = \\frac{1}{\\abs{\\Sett{S}_0 }} \\sum_{\\\\Sett{S}_0 \\cup \\Sett{S}_1}   (1 - S_i)  \\wh{\\mu}^{}(X_i) +    \\frac{S_i(1 - \\wh{\\rho}(X_i))}{\\wh{\\rho}(X_i)}       \\frac{\\Indic{= }}{\\wh{\\pi}^(X_i)} (Y_i - \\wh{\\mu}^(X_i) ) \\]","code":"ateGT(y, a, X, s, target = \"transport\", estimator = \"AISW\")"},{"path":"/articles/tech_details.html","id":"calibration-weighting-cw","dir":"Articles","previous_headings":"Estimators > Reweighting","what":"Calibration Weighting (CW)","title":"Technical Details : identification and estimation","text":"Estimating propensity score taking inverse can sub-optimal behaviour certain observations small values, propensity model misspecified. Alternatively, may seek directly solve balancing weights guarantee balance \\[ \\sum_{\\\\Sett{S}_0} \\omega_i c_{ij}(X_ij) \\approx \\sum_{\\\\Sett{S}_0 \\cup \\Sett{S}_1} c_{ij}(X_ij) \\] specified moment functions \\(c_{ij}\\) (typically identity functions, implies means higher moments) experimental sample target sample (either overall extrapolation sample). weights ‘automatic’ sense require choice parametric model outcome propensity model, shown exhibit excellent performance finite samples. popular calibration weight entropy weight, involves solving following (convex) program \\[\\begin{align*}   \\max_{\\mathbf{w}} H(w) &= - \\sum_{: \\\\mathcal{S}_1 } w_i \\log w_i \\\\   \\text{Balance constraints:} & \\sum_{: \\\\mathcal{S}_1} w_i c_{ri}(\\mathbf{X}_i) =       m_r(X_j) \\text{ } r \\1, \\dots, R   \\\\   \\text{Proper weights:} & \\sum_{: \\\\mathcal{S}_1} w_i = 1 \\; \\; \\text{}   w_i \\geq 0 \\; \\forall \\; \\{: \\\\mathcal{S}_1 \\} \\end{align*}\\] convex program \\(R\\) balance constraints seek equate functions \\(c_{ri}(X_i)\\) source sample \\(\\mathcal{S}_1\\) target value \\(m_r\\) (may different overall extrapolation sample). Solving dual problem computationally fast scales well. Entropy balancing also double-robust: consistent outcome model linear selection model log-linear.","code":""},{"path":"/articles/tech_details.html","id":"generalisation-4","dir":"Articles","previous_headings":"Estimators > Reweighting > Calibration Weighting (CW)","what":"generalisation","title":"Technical Details : identification and estimation","text":"Set \\(m_r(X_j)\\) average covariate values overall sample. Solve ebal program obtain weights \\(\\wh{\\omega}_i\\) \\[ \\wh{\\phi}_a = \\sum_{\\\\Sett{S}_0 \\cup \\Sett{S}_1} \\wh{\\omega}_i \\frac{\\Indic{= }}{\\wh{\\pi}^(X)} Y \\]","code":"ateGT(y, a, X, s, target = \"generalize\", estimator = \"CW\")$res ##      parameter       est         se     ci.ll     ci.ul pval ## 1      E{Y(0)} 0.4742417 0.03478469 0.4060638 0.5424197    0 ## 2      E{Y(1)} 1.8476831 0.05210291 1.7455614 1.9498048    0 ## 3 E{Y(1)-Y(0)} 1.3734414 0.06517506 1.2456983 1.5011845    0"},{"path":"/articles/tech_details.html","id":"transportation-4","dir":"Articles","previous_headings":"Estimators > Reweighting > Calibration Weighting (CW)","what":"transportation","title":"Technical Details : identification and estimation","text":"Set \\(m_r(X_j)\\) average covariate values extrapolation sample. Solve ebal program obtain weights \\(\\wh{\\omega}_i^1\\) \\[ \\wh{\\phi}_a = \\sum_{\\\\Sett{S}_0 \\cup \\Sett{S}_1} \\wh{\\omega}_i^1 \\frac{\\Indic{= }}{\\wh{\\pi}^(X)} Y \\]","code":"ateGT(y, a, X, s, target = \"transport\", estimator = \"CW\")$res ##      parameter       est         se     ci.ll     ci.ul pval ## 1      E{Y(0)} 0.4629304 0.04965513 0.3656064 0.5602545    0 ## 2      E{Y(1)} 2.3293385 0.09136245 2.1502681 2.5084089    0 ## 3 E{Y(1)-Y(0)} 1.8664081 0.10587890 1.6588854 2.0739307    0"},{"path":"/articles/tech_details.html","id":"augmented-calibration-weighting-acw","dir":"Articles","previous_headings":"Estimators > Reweighting","what":"Augmented Calibration weighting (ACW)","title":"Technical Details : identification and estimation","text":"AISW, may want combine ICW OM approaches robust approach works well either weights \\(\\mu\\) correctly specified","code":""},{"path":"/articles/tech_details.html","id":"generalisation-5","dir":"Articles","previous_headings":"Estimators > Reweighting > Augmented Calibration weighting (ACW)","what":"generalisation","title":"Technical Details : identification and estimation","text":"\\[ \\Exp{Y^} = \\mu(, X)  +       \\omega       \\frac{\\Indic{= }}{\\pi^(X)} (Y - \\mu^(X) ) \\] estimator ‘augments’ outcome model weighted average residuals (\\(Y - \\mu(\\cdot)\\)). Sample analogue: \\[ \\wh{\\phi}_a = \\frac{1}{\\abs{\\Sett{S}_0 \\cup \\Sett{S}_1}} \\sum_{\\\\Sett{S}_0 \\cup \\Sett{S}_1} \\wh{\\mu}^{}(X_i) + \\wh{\\omega}_i \\frac{\\Indic{= }}{\\wh{\\pi}^(X)} (Y - \\wh{\\mu}^(X) ) \\]","code":"ateGT(y, a, X, s, target = \"generalize\", estimator = \"ACW\")$res ##      parameter      est         se     ci.ll     ci.ul pval ## 1      E{Y(0)} 0.312567 0.01732877 0.2786026 0.3465314    0 ## 2      E{Y(1)} 1.442708 0.01776030 1.4078982 1.4775186    0 ## 3 E{Y(1)-Y(0)} 1.130141 0.02374645 1.0835983 1.1766844    0"},{"path":"/articles/tech_details.html","id":"transportation-5","dir":"Articles","previous_headings":"Estimators > Reweighting > Augmented Calibration weighting (ACW)","what":"transportation","title":"Technical Details : identification and estimation","text":"\\[ \\Exp{Y^| S = 0} = \\mu(, X)  +       \\omega^1       \\frac{\\Indic{= }}{\\pi^(X)} (Y - \\mu^(X) ) \\] Sample analogue: \\[ \\wh{\\phi}^1_a = \\frac{1}{\\abs{\\Sett{S}_0 }} \\sum_{\\\\Sett{S}_0} \\wh{\\mu}^{}(X_i) + \\wh{\\omega}_i^1 \\frac{\\Indic{= }}{\\wh{\\pi}^(X)} (Y - \\wh{\\mu}^(X) ) \\]","code":"ateGT(y, a, X, s, target = \"transport\", estimator = \"ACW\")$res ##      parameter       est         se     ci.ll     ci.ul pval ## 1      E{Y(0)} 0.2318273 0.03578722 0.1616844 0.3019703    0 ## 2      E{Y(1)} 1.8084866 0.03650755 1.7369318 1.8800414    0 ## 3 E{Y(1)-Y(0)} 1.5766593 0.05009439 1.4784743 1.6748443    0"},{"path":"/articles/tech_details.html","id":"surrogate-models","dir":"Articles","previous_headings":"Estimators","what":"Surrogate Models","title":"Technical Details : identification and estimation","text":"surrogate outcomes (outcomes measured baseline randomisation endline), may incorporated analysis produce precision gains (since outcomes tend correlated time). additional assumption required consistency Cohort \\(S\\) Missing Random (MAR): \\(S \\indep Y^| (X , = , Z^)\\)","code":""},{"path":"/articles/tech_details.html","id":"augmented-inverse-selection-weighting-with-surrogates-aisws","dir":"Articles","previous_headings":"Estimators > Surrogate Models","what":"Augmented Inverse Selection Weighting with Surrogates (AISWS)","title":"Technical Details : identification and estimation","text":"Surrogates can incorporated AISW estimator additional debiasing piece","code":""},{"path":"/articles/tech_details.html","id":"generalisation-6","dir":"Articles","previous_headings":"Estimators > Surrogate Models > Augmented Inverse Selection Weighting with Surrogates (AISWS)","what":"generalisation","title":"Technical Details : identification and estimation","text":"\\[ \\Exp{Y^} = \\mu(, X)  +       \\frac{1}{\\rho(X)}       \\frac{\\Indic{= }}{\\pi^(X)} (Y - \\mu^(X) ) +       \\frac{\\Indic{= }}{\\pi^(X)} (\\nu^(Z, X) - \\mu^(X)) \\] estimator ‘augments’ outcome model two weighted averages residuals \\(Y - \\mu(\\cdot)\\) \\(\\nu(\\cdot) - \\mu(\\cdot)\\). Sample analogue: \\[ \\wh{\\phi}_a = \\frac{1}{\\abs{\\Sett{S}_0 \\cup \\Sett{S}_1}} \\sum_{\\\\Sett{S}_0 \\cup \\Sett{S}_1} \\wh{\\mu}^{}(X_i) +   \\frac{S_i}{\\wh{\\rho}(X_i)}       \\frac{\\Indic{= }}{\\wh{\\pi}^(X_i)}     (Y_i - \\wh{\\mu}^(X_i) ) +   \\frac{\\Indic{= }}{\\wh{\\pi}^(X_i)}   (\\wh{\\nu}^(X_i, Z_i) - \\wh{\\mu}^(X_i)) \\]","code":""},{"path":"/articles/tech_details.html","id":"transportation-6","dir":"Articles","previous_headings":"Estimators > Surrogate Models > Augmented Inverse Selection Weighting with Surrogates (AISWS)","what":"transportation","title":"Technical Details : identification and estimation","text":"\\[ \\Exp{Y^} = \\mu(, X)  +       \\frac{1 - \\rho(X)}{\\rho(X)}       \\frac{\\Indic{= }}{\\pi^(X)} (Y - \\mu^(X) ) +       \\frac{\\Indic{= }}{\\pi^(X)} (\\nu^(Z, X) - \\mu^(X)) \\] Sample analogue: \\[ \\wh{\\phi}_a = \\frac{1}{\\abs{\\Sett{S}_0 \\cup \\Sett{S}_1}} \\sum_{\\\\Sett{S}_0 \\cup \\Sett{S}_1} \\wh{\\mu}^{}(X_i) +   \\frac{S_i(1- \\wh{\\rho}(X_i))}{\\wh{\\rho}(X_i)}       \\frac{\\Indic{= }}{\\wh{\\pi}^(X_i)}     (Y_i - \\wh{\\mu}^(X_i) ) +   \\frac{\\Indic{= }}{\\wh{\\pi}^(X_i)}   (\\wh{\\nu}^(X_i, Z_i) - \\wh{\\mu}^(X_i)) \\]","code":""},{"path":"/articles/tech_details.html","id":"estimation-function-for-aggregate-data-for-target-atecal","dir":"Articles","previous_headings":"","what":"Estimation Function for aggregate data for target: ateCAL","title":"Technical Details : identification and estimation","text":"simulate data withold individual level data generalization / target sample. Calibration Augmented calibration estimators can used generalization transportation setting target moments appropriate sample.","code":"# generate RCT with 10k obs n <- 10000 p <- 10 treat.prob <- 0.5 # random assignment a <- rbinom(n, 1, treat.prob) X <- matrix(rnorm(n * p, 0, 2), n, p) TAU <- 1 / (1 + exp(-X[, 3])) # step fn in outcome model y0 <- X[, 1] + X[, 2] + X[, 5] + pmax(X[, 7], 2) y1 <- a * TAU + y0 + rnorm(n) y <- (1 - a) * y0 + a * y1 # selection model misspecified selscore <- X[, 1] - 0.5 * X[, 3] + pmax(X[, 4], 0) s <- rbinom(n, 1, plogis(selscore)) |> as.logical() # now, don't feed the algorithm a, s, X for the s = 0 group"},{"path":"/articles/tech_details.html","id":"generalisation-7","dir":"Articles","previous_headings":"Estimation Function for aggregate data for target: ateCAL","what":"generalisation","title":"Technical Details : identification and estimation","text":"Truth: 0.4998786","code":"target_moments <- colMeans2(X) ateCAL(y[s == 1], a[s == 1], X[s == 1, ],     treatProb = treat.prob,     targetMoments = target_moments,     estimator = \"ACW\", noi = F )$res ##      parameter       est         se     ci.ll     ci.ul pval ## 1      E{Y(0)} 3.0407581 0.04451131 2.9535160 3.1280003    0 ## 2      E{Y(1)} 3.4773413 0.05296667 3.3735267 3.5811560    0 ## 3 E{Y(1)-Y(0)} 0.4365832 0.03225989 0.3733538 0.4998126    0 ateCAL(y[s == 1], a[s == 1], X[s == 1, ],     treatProb = treat.prob,     targetMoments = target_moments,     estimator = \"CW\", noi = F )$res ##      parameter       est        se    ci.ll     ci.ul  pval ## 1      E{Y(0)} 3.8525818 0.1011644 3.654300 4.0508640 0.000 ## 2      E{Y(1)} 4.2450770 0.1072732 4.034821 4.4553325 0.000 ## 3 E{Y(1)-Y(0)} 0.3924952 0.1650848 0.068929 0.7160614 0.017"},{"path":"/articles/tech_details.html","id":"transportation-7","dir":"Articles","previous_headings":"Estimation Function for aggregate data for target: ateCAL","what":"transportation","title":"Technical Details : identification and estimation","text":"Truth: 0.5918173","code":"target_moments <- colMeans2(X[s == 0, ]) ateCAL(y[s == 1], a[s == 1], X[s == 1, ],     treatProb = treat.prob,     targetMoments = target_moments,     estimator = \"ACW\", noi = F )$res ##      parameter       est         se     ci.ll     ci.ul pval ## 1      E{Y(0)} 3.0407845 0.04625990 2.9501151 3.1314539    0 ## 2      E{Y(1)} 3.4697880 0.06309539 3.3461210 3.5934550    0 ## 3 E{Y(1)-Y(0)} 0.4290035 0.04857097 0.3338044 0.5242026    0 ateCAL(y[s == 1], a[s == 1], X[s == 1, ],     treatProb = treat.prob,     targetMoments = target_moments,     estimator = \"CW\", noi = F )$res ##      parameter       est        se     ci.ll     ci.ul  pval ## 1      E{Y(0)} 3.2393090 0.1395562 2.9657789 3.5128392 0.000 ## 2      E{Y(1)} 3.7852859 0.1481834 3.4948464 4.0757255 0.000 ## 3 E{Y(1)-Y(0)} 0.5459769 0.2134626 0.1275903 0.9643635 0.011"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Apoorva Lal. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Lal (2022). causalTransportR: Machine learning methods estimating bridging causal effects. R package version 0.1.","code":"@Manual{,   title = {causalTransportR: Machine learning methods for estimating and bridging causal effects},   author = {Apoorva Lal},   year = {2022},   note = {R package version 0.1}, }"},{"path":"/index.html","id":"causaltransportr--a-package-for-bridging-causal-effects-to-new-or-general-populations","dir":"","previous_headings":"","what":"Machine learning methods for estimating and bridging causal effects","title":"Machine learning methods for estimating and bridging causal effects","text":"causalTransportR implements number estimators generalize transport causal effects reweighting doubly-robust score functions transformations selection scores. nuisance functions cross-fit using fast supervised learning algorithms.","code":""},{"path":"/index.html","id":"estimators","dir":"","previous_headings":"","what":"Estimators","title":"Machine learning methods for estimating and bridging causal effects","text":"ateCAL function implements following estimators aggregating estimates individual marginal means different marginal X distributions generalization transportation case. μ, π, ρ nuisance parameters fit using ML.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Machine learning methods for estimating and bridging causal effects","text":"","code":"# install.packages(\"remotes\") # if remotes isn't installed remotes::install_github(\"Netflix-Skunkworks/causalTransportR\")"},{"path":"/index.html","id":"reference","dir":"","previous_headings":"","what":"Reference","title":"Machine learning methods for estimating and bridging causal effects","text":"Lal, Apoorva, Wenjing Zheng, Simon Ejdemyr, “Framework Generalization Transportation Causal Estimates Covariate Shift”, 2022","code":""},{"path":"/reference/aipw.html","id":null,"dir":"Reference","previous_headings":"","what":"AIPW estimation under unconfoundedness — aipw","title":"AIPW estimation under unconfoundedness — aipw","text":"Simplified version `ateGT` selection indicator. Imputes counterfactual outcome observation treament $$Y^= \\frac{= }{\\pi^(X)} (Y - \\mu^(X)) + \\mu^(X) $$ Average treatment effects defined averages differences counterfactual outcomes \\(Y^- Y^{'}\\). returns marginal means, causal contrasts, influence-function based variance estimates. Uses underlying nuisance function fitter `ateGT` missing data surrogates.","code":""},{"path":"/reference/aipw.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"AIPW estimation under unconfoundedness — aipw","text":"","code":"aipw(   y,   a,   X,   treatProb = NULL,   nuisMod = c(\"rlm\", \"rf\"),   estimator = c(\"AIPW\", \"IPW\", \"OM\"),   hajekize = FALSE,   separateMus = TRUE,   glmnet_lamchoice = \"lambda.min\",   glmnet_alpha = 1,   glmnet_rho_family = \"binomial\",   glmnet_pi_family = \"binomial\",   glmnet_mu_family = \"gaussian\",   glmnet_parl = FALSE,   grf_tunerf = \"none\",   noi = FALSE )"},{"path":"/reference/aipw.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"AIPW estimation under unconfoundedness — aipw","text":"y outcome vector (may contain missings ; missings must correspond s = 0) treatment vector (missings; can relaxed tinkering) X covariate matrix (missings) treatProb propensity score vector (length n_treatment) matrix (n_treatment X n_obs), latter covariate adaptive designs; must sum 1. NULL default, pscore fitted. provided, propensity score fit. discrete covariates, estimated propensity score advisable even treatment randomized. nuisMod one c(\"rlm\", \"rf\") : choose fit nuisance functions (cross-fit). estimator one c(\"AIPW\", \"IPW\", \"OM\"). default AIPW. hajekize boolean whether divide inverse probability weights term treatment level sum weights treatment level. guards instability large weights extremely small selection propensity scores. separateMus boolean whether fit separate outcome models treatment group single pooled model. former recommended default, pooled model may fit data scarce / computation burdensome. glmnet_lamchoice choice lambda (shrinkage parameter) regularized linear regressions. relevant nuisMod == \"rlm\" glmnet_alpha [0, 1], choice alpha glmnet. 1 (default) corresponds L1 regularization (LASSO) 0 corresponds L2 regularization (ridge), intermediate values correspond mix two (elastic net) glmnet_rho_family GLM family selection model. \"binomial\" default can safely switched \"gaussian\" linear probability models discrete covariates faster compute glmnet_pi_family GLM family propensity model. \"binomial\" default can safely switched \"gaussian\" linear probability models discrete covariates faster compute glmnet_mu_family GLM family outcome model. Gaussian default. glmnet_parl Boolean parallelization glmnet. Need enable parallelized cluster beforehand. grf_tunerf Tune rf hyperparameters? Passed grf's regression forest. Use '' hyperparameter tuning. noi boolean printing marginal means causal contrasts table (gets returned anyway). default.","code":""},{"path":"/reference/aipw.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"AIPW estimation under unconfoundedness — aipw","text":"list containing treatment effects table , nuisance function estimates, influence function values","code":""},{"path":"/reference/aipw.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"AIPW estimation under unconfoundedness — aipw","text":"","code":"# simulation with no selection bias (generate by passing null function for selection) df2 = selDGP(selF = NULL) df2$tau |> mean() # true effect : sinh(1) #> [1] 1.173417 # lasso aipw(df2$y, df2$a, df2$X, noi = FALSE, nuisMod = \"rlm\")$res #>      parameter       est         se     ci.ll    ci.ul pval #> 1      E{Y(0)} 0.9406017 0.04311354 0.8560991 1.025104    0 #> 2      E{Y(1)} 2.2060840 0.03603019 2.1354648 2.276703    0 #> 3 E{Y(1)-Y(0)} 1.2654823 0.03405089 1.1987425 1.332222    0 # random forest aipw(df2$y, df2$a, df2$X, noi = FALSE, nuisMod = \"rf\")$res #>      parameter       est         se     ci.ll    ci.ul pval #> 1      E{Y(0)} 0.9831722 0.04122702 0.9023673 1.063977    0 #> 2      E{Y(1)} 2.1909966 0.03574042 2.1209453 2.261048    0 #> 3 E{Y(1)-Y(0)} 1.2078243 0.02891910 1.1511429 1.264506    0"},{"path":"/reference/ateCAL.html","id":null,"dir":"Reference","previous_headings":"","what":"ateCAL: reweight experimental results to match external sample — ateCAL","title":"ateCAL: reweight experimental results to match external sample — ateCAL","text":"Generalization transportation estimator method  moments weighting. Imputes counterfactual outcome observation treament $$Y^= \\omega \\frac{= }{\\pi^(X)} (Y - \\mu^(X)) + \\mu^(X) $$ \\(\\omega\\) solved using entropy balancing balance target moments.","code":""},{"path":"/reference/ateCAL.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ateCAL: reweight experimental results to match external sample — ateCAL","text":"","code":"ateCAL(   y,   a,   X,   targetMoments,   treatProb = NULL,   nuismod = c(\"rlm\", \"rf\"),   estimator = c(\"ACW\", \"CW\"),   glmnet_lamchoice = \"lambda.min\",   glmnet_alpha = 1,   separateMus = TRUE,   noi = FALSE )"},{"path":"/reference/ateCAL.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"ateCAL: reweight experimental results to match external sample — ateCAL","text":"y outcome vector [length n] treatment vector [length n] X covariate matrix [n X k] targetMoments k-vector target moments (means overall sample generalization, means target sample transportation treatProb treatment probability, non-null results pscore fit nuismod one c(\"rlm\", \"rf\") : choose fit nuisance functions (cross-fit)  outcome models estimator one c(\"CW\", \"ACW\"). Calibration weights (CW), fit set balancing weights reweights sample match target sample moments. ACW augments outcome model (default) glmnet_lamchoice choice lambda (shrinkage parameter) regularized linear regressions. relevant nuismod == \"rlm\" glmnet_alpha [0, 1], choice alpha glmnet. 1 (default) corresponds L1 regularization (LASSO) 0 corresponds L2 regularization (ridge), intermediate values correspond mix two (elastic net) separateMus boolean whether fit separate outcome models treatment group single pooled model. former recommended default, pooled model may fit data scarce / computation burdensome. noi boolean printing marginal means causal contrasts table (gets returned anyway). default.","code":""},{"path":"/reference/ateCAL.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"ateCAL: reweight experimental results to match external sample — ateCAL","text":"list containing treatment effects table , nuisance function estimates, influence function values","code":""},{"path":"/reference/ateCAL.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"ateCAL: reweight experimental results to match external sample — ateCAL","text":"","code":"df = selDGP(n = 10000) id = with(df, s == 1) X = df$X[id == 1, ]; y = df$y[id == 1]; a = df$a[id == 1] Xtar = colMeans(df$X[!id, ]) ateCAL(y, a, X, targetMoments = Xtar, estimator = \"ACW\") %>% summary() #>      parameter       est         se     ci.ll    ci.ul pval #> 1      E{Y(0)} 1.0769520 0.10987596 0.8615951 1.292309    0 #> 2      E{Y(1)} 1.9754591 0.08484183 1.8091691 2.141749    0 #> 3 E{Y(1)-Y(0)} 0.8985071 0.12300151 0.6574242 1.139590    0"},{"path":"/reference/ateGT.html","id":null,"dir":"Reference","previous_headings":"","what":"Omnibus function for ATE estimation for generalization and transportation — ateGT","title":"Omnibus function for ATE estimation for generalization and transportation — ateGT","text":"Augmented IPW, generalization, transport estimator ML cross-fitting nuisance functions. Imputes counterfactual outcome observation treament $$Y^= \\omega \\frac{= }{\\pi^(X)} (Y - \\mu^(X)) + \\mu^(X) $$ \\(\\omega\\) 1 observations sample selection, therefore doubly-robust Augmented Inverse Propensity Weighting (AIPW) estimator. S supplied, argument 'target' used fit either generalization transportation estimator, corresponds \\(\\omega = S/\\rho(X)\\) \\(\\omega = (S (1-\\rho(X)) /\\rho(X)\\) respectively. surrogate vector \\(Z\\) supplied, additional residual piece \\(/\\pi(X)(\\hat{\\nu}(X, Z) -  \\hat{\\mu}(X))\\)  added influence function. Average treatment effects defined averages differences counterfactual outcomes \\(Y^- Y^{'}\\).","code":""},{"path":"/reference/ateGT.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Omnibus function for ATE estimation for generalization and transportation — ateGT","text":"","code":"ateGT(   y,   a,   X,   s = NULL,   treatProb = NULL,   Z = NULL,   nuisMod = c(\"rlm\", \"rf\"),   target = c(\"generalize\", \"transport\", \"insample\"),   estimator = c(\"AISW\", \"ISW\", \"OM\", \"CW\", \"ACW\"),   hajekize = FALSE,   separateMus = TRUE,   glmnet_lamchoice = \"lambda.min\",   glmnet_alpha = 1,   glmnet_rho_family = \"binomial\",   glmnet_pi_family = \"binomial\",   glmnet_mu_family = \"gaussian\",   glmnet_parl = FALSE,   grf_tuneRf = \"none\",   noi = FALSE )"},{"path":"/reference/ateGT.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Omnibus function for ATE estimation for generalization and transportation — ateGT","text":"y outcome vector (may contain missings ; missings must correspond s = 0) treatment vector (missings; can relaxed tinkering) X covariate matrix (missings) s selection vector, NULL default (missings, 1 corresponds nonmissing y; 0 corresponds missing y). May omitted target \"insample\" . treatProb propensity score vector (length n_treatment) matrix (n_treatment X n_obs), latter covariate adaptive designs; must sum 1. NULL default, pscore fitted. provided, propensity score fit. discrete covariates, estimated propensity score advisable even treatment randomized. Z surrogate matrix, NULL default (missings). nonmissing, surrogate influence function (Kallus Mao 2020) used compute treatment effects. nuisMod one c(\"rlm\", \"rf\") : choose fit nuisance functions (cross-fit). target one c(\"generalize\", \"transport\", \"insample\") estimand target. \"generalize\" generalizes (quasi)experimental estimates complete data (S == 1) overall sample (S == 0 S == 1). \"transport\" transports estimates S == 1 sample S == 0 sample. \"insample\" estimates causal effects S == 1 sample (.e. conventional quasi/experimental estimation). estimator one c(\"AISW\", \"ISW\", \"OM\", \"CW\", \"ACW\"). default augmented inverse selection weighting estimator, augments inverse selection weighting estimator (ISW) outcome model (OM). ACW calibration weights (CW), fit set entropy balancing weights reweights sample match target sample moments. hajekize boolean whether divide inverse probability weights term treatment level sum weights treatment level. guards instability large weights extremely small selection propensity scores. separateMus boolean whether fit separate outcome models treatment group single pooled model. former recommended default, pooled model may fit data scarce / computation burdensome. glmnet_lamchoice choice lambda (shrinkage parameter) regularized linear regressions. relevant nuisMod == \"rlm\" glmnet_alpha [0, 1], choice alpha glmnet. 1 (default) corresponds L1 regularization (LASSO) 0 corresponds L2 regularization (ridge), intermediate values correspond mix two (elastic net) glmnet_rho_family GLM family selection model. \"binomial\" default can safely switched \"gaussian\" linear probability models discrete covariates faster compute glmnet_pi_family GLM family propensity model. \"binomial\" default can safely switched \"gaussian\" linear probability models discrete covariates faster compute glmnet_mu_family GLM family outcome model. Gaussian default. glmnet_parl Boolean parallelization glmnet. Need enable parallelized cluster beforehand. grf_tuneRf Tune rf hyperparameters? Passed grf's regression forest. Use '' hyperparameter tuning. noi boolean printing marginal means causal contrasts table (gets returned anyway). default.","code":""},{"path":"/reference/ateGT.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Omnibus function for ATE estimation for generalization and transportation — ateGT","text":"list containing treatment effects table , nuisance function estimates, influence function values","code":""},{"path":"/reference/ateGT.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Omnibus function for ATE estimation for generalization and transportation — ateGT","text":"Bia, M., M. Huber, L. Lafférs. (2020): “Double Machine Learning Sample Selection Models,” arXiv [econ.EM],. Dahabreh, . J., S. E. Robertson, E. J. Tchetgen, E. . Stuart, M. . Hernán. (2019): “Generalizing causal inferences individuals randomized trials trial-eligible individuals,” Biometrics, 75, 685–94. Hirshberg, D. ., . Maleki, J. R. Zubizarreta. (2019): “Minimax Linear Estimation Retargeted Mean,” arXiv [math.ST],. Kallus, N., X. Mao. (2020): “Role Surrogates Efficient Estimation Treatment Effects Limited Outcome Data,” arXiv [stat.ML],.","code":""},{"path":[]},{"path":"/reference/ateGTreg.html","id":null,"dir":"Reference","previous_headings":"","what":"Generalization AISW using fixest fixed effects regressions — ateGTreg","title":"Generalization AISW using fixest fixed effects regressions — ateGTreg","text":"Augmented IPW, generalization, transport estimator fast fixest regressions computation `data.table` (1.14.5), regularization, sample splitting. Imputes counterfactual outcome observation treament \\(Y^= \\frac{S}{\\rho(X)} \\frac{= }{\\pi^(X)} (Y - \\mu^(X)) + \\mu^(X) \\) first term 1 observations sample selection, therefore doubly-robust Augmented Inverse Propensity Weighting (AIPW) estimator. S supplied, argument 'target' used fit either generalization transportation estimator. Recommended nonparametric/bayesian bootstrap inference.","code":""},{"path":"/reference/ateGTreg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generalization AISW using fixest fixed effects regressions — ateGTreg","text":"","code":"ateGTreg(   d,   yn = \"y\",   an = \"a\",   sn = NULL,   xn = \"1\",   fe = \"0\",   target = c(\"generalize\", \"transport\", \"insample\") )"},{"path":"/reference/ateGTreg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generalization AISW using fixest fixed effects regressions — ateGTreg","text":"d data.table yn outcome name treatment indicator name sn selection indicator name (null default - fits AIPW regression) xn list covariates (default intercept) fe list fixed effects (default 0) target estimand (generalization / transportation/ insample)","code":""},{"path":"/reference/ateGTreg.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generalization AISW using fixest fixed effects regressions — ateGTreg","text":"generalization effect influence functions","code":""},{"path":"/reference/dfmTest.html","id":null,"dir":"Reference","previous_headings":"","what":"Omnibus test of systematic treatment effect heterogeneity (Ding, Feller, Miratrix 2018) — dfmTest","title":"Omnibus test of systematic treatment effect heterogeneity (Ding, Feller, Miratrix 2018) — dfmTest","text":"Omnibus test treatment effect heterogeneity along specified dimensions X. Implemented regressing individual treatment effect covariates joint test covariate coefficients = 0","code":""},{"path":"/reference/dfmTest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Omnibus test of systematic treatment effect heterogeneity (Ding, Feller, Miratrix 2018) — dfmTest","text":"","code":"dfmTest(y, a, X)"},{"path":"/reference/dfmTest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Omnibus test of systematic treatment effect heterogeneity (Ding, Feller, Miratrix 2018) — dfmTest","text":"y outcome vector treament vector X covariate matrix","code":""},{"path":"/reference/dfmTest.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Omnibus test of systematic treatment effect heterogeneity (Ding, Feller, Miratrix 2018) — dfmTest","text":"vector aith chi-squared statistic p-value systematic heterogeneity","code":""},{"path":"/reference/dfmTest.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Omnibus test of systematic treatment effect heterogeneity (Ding, Feller, Miratrix 2018) — dfmTest","text":"Ding, P., . Feller, L. Miratrix. (2019): “Decomposing Treatment Effect Variation,” Journal American Statistical Association, 114, 304–17.","code":""},{"path":"/reference/eb_solve_dual.html","id":null,"dir":"Reference","previous_headings":"","what":"Entropy balancing by solving dual — eb_solve_dual","title":"Entropy balancing by solving dual — eb_solve_dual","text":"Entropy balancing solving dual","code":""},{"path":"/reference/eb_solve_dual.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Entropy balancing by solving dual — eb_solve_dual","text":"","code":"eb_solve_dual(   X1m,   X0,   coefs = NULL,   base.weight = NULL,   maxIterations = 200L,   constraint.tolerance = 1,   printLevel = 0 )"},{"path":"/reference/eb_solve_dual.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Entropy balancing by solving dual — eb_solve_dual","text":"X1m K-vector target means X0 NxK matrix covariates coefs starting coefs solution base.weight = [NULL] n-vector baseline weights maxIterations [200] stopping rule constraint.tolerance [1] value constraint threshold printLevel [0, 1, 2, 3] 0 silent, 1 reports success, 2 3 noisy (debugging) sparsify [T/F] (progress) run Newton-Raphson sparse matrix classes Matrix package","code":""},{"path":"/reference/interSparseM.html","id":null,"dir":"Reference","previous_headings":"","what":"prepare sparse X matrix with interactions — interSparseM","title":"prepare sparse X matrix with interactions — interSparseM","text":"Prepare sparse matrix  interactions.","code":""},{"path":"/reference/interSparseM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"prepare sparse X matrix with interactions — interSparseM","text":"","code":"interSparseM(data, k, corr_cut = 0.9)"},{"path":"/reference/interSparseM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"prepare sparse X matrix with interactions — interSparseM","text":"data data table / dataframe k order interactions: defaults pairwise interactions corr_cut cutoff correlation threshold drop one vars (default = 0.9)","code":""},{"path":"/reference/interSparseM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"prepare sparse X matrix with interactions — interSparseM","text":"matrix interactions","code":""},{"path":"/reference/mMscale.html","id":null,"dir":"Reference","previous_headings":"","what":"min-max scale (maps continuous variable to [0, 1]) — mMscale","title":"min-max scale (maps continuous variable to [0, 1]) — mMscale","text":"min-max scale (maps continuous variable [0, 1])","code":""},{"path":"/reference/mMscale.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"min-max scale (maps continuous variable to [0, 1]) — mMscale","text":"","code":"mMscale(X)"},{"path":"/reference/mMscale.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"min-max scale (maps continuous variable to [0, 1]) — mMscale","text":"X vector","code":""},{"path":"/reference/mlRate.html","id":null,"dir":"Reference","previous_headings":"","what":"Flexible covariate adjustment using ML predictions — mlRate","title":"Flexible covariate adjustment using ML predictions — mlRate","text":"First fits predictive model covariates outcome \\(Y\\) construct fitted values \\(g(X)\\). , runs ML-based version Lin (2013) regression predictions \\(g(X)\\). \\(Y = \\beta_0 + \\beta_1 + \\beta_2 g(X) + \\beta_4 (\\times (g(X) - \\bar{g}(X) )\\) treatment interacted predictions g(). dimension reduced veresion \\(X\\)s instead using full matrix. Treatment effect \\(\\beta_1\\) heteroskedasticity robust variance estimate asymptotically bounded naive Neyman variance typically lower covariates explain substantial variation Y.","code":""},{"path":"/reference/mlRate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Flexible covariate adjustment using ML predictions — mlRate","text":"","code":"mlRate(   y,   a,   X,   nuisMod = c(\"rlm\", \"rf\"),   glmnet_lamchoice = \"lambda.min\",   glmnet_alpha = 1,   glmnet_mu_family = \"gaussian\",   glmnet_parl = FALSE,   tuneRf = \"none\",   noi = FALSE )"},{"path":"/reference/mlRate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Flexible covariate adjustment using ML predictions — mlRate","text":"y outcome vector treatment dummy vector X covariate matrix nuisMod ML algorithm fit nuisance function. Defaults glmnet, can also use generalized random forests. glmnet_lamchoice choice lambda (shrinkage parameter) regularized linear regressions. relevant nuisMod == \"rlm\" glmnet_alpha [0, 1], choice alpha glmnet. 1 (default) corresponds L1 regularization (LASSO) 0 corresponds L2 regularization (ridge), intermediate values correspond mix two (elastic net) glmnet_mu_family GLM family outcome model. Gaussian default. glmnet_parl Boolean parallelization glmnet. Need enable parallelized cluster beforehand. tuneRf boolean whether tune RF noi Boolean noisy","code":""},{"path":"/reference/mlRate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Flexible covariate adjustment using ML predictions — mlRate","text":"treatment effect SE","code":""},{"path":"/reference/mlRate.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Flexible covariate adjustment using ML predictions — mlRate","text":"Guo, Y., Coey, D., Konutgan, M., Li, W., Schoener, C., & Goldman, M. (2021). Machine learning variance reduction online experiments. Advances Neural Information Processing Systems, 34, 8637-8648. Lin, Winston. \"Agnostic notes regression adjustments experimental data: Reexamining Freedman’s critique.\" Annals Applied Statistics 7.1 (2013): 295-318.","code":""},{"path":"/reference/plot.ategt.html","id":null,"dir":"Reference","previous_headings":"","what":"SMD balance plot method - plots balance across covariates and gains from reweighting — plot.ategt","title":"SMD balance plot method - plots balance across covariates and gains from reweighting — plot.ategt","text":"SMD balance plot method - plots balance across covariates gains reweighting","code":""},{"path":"/reference/plot.ategt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"SMD balance plot method - plots balance across covariates and gains from reweighting — plot.ategt","text":"","code":"# S3 method for ategt plot(fit, X)"},{"path":"/reference/plot.ategt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"SMD balance plot method - plots balance across covariates and gains from reweighting — plot.ategt","text":"fit ategt object covariate matrix","code":""},{"path":"/reference/plot.ategt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SMD balance plot method - plots balance across covariates and gains from reweighting — plot.ategt","text":"","code":"sim = selDGP(selF = \\(x) -3 * x[5] + x[2] + 3 * x[10]) fit = with(sim, ateGT(y = y, a = a, X = X, s = s)) fit %>% summary #>      parameter       est         se     ci.ll    ci.ul pval #> 1      E{Y(0)} 0.9212635 0.09991788 0.7254244 1.117103    0 #> 2      E{Y(1)} 2.3017811 0.09478780 2.1159970 2.487565    0 #> 3 E{Y(1)-Y(0)} 1.3805176 0.13039910 1.1249354 1.636100    0 mean(sim$tau) #> [1] 1.171168 plot(fit, sim$X)"},{"path":"/reference/polySieveM.html","id":null,"dir":"Reference","previous_headings":"","what":"prepare X matrix with flexible interactions and polynomials — polySieveM","title":"prepare X matrix with flexible interactions and polynomials — polySieveM","text":"Prepare matrix polynomial basis expansion /interactions.","code":""},{"path":"/reference/polySieveM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"prepare X matrix with flexible interactions and polynomials — polySieveM","text":"","code":"polySieveM(   dat,   dummies = NULL,   continuouses = NULL,   corr_cut = 0.9,   k = 2,   m = 2,   raw = FALSE )"},{"path":"/reference/polySieveM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"prepare X matrix with flexible interactions and polynomials — polySieveM","text":"dat data table / dataframe dummies Names dummy vars (null default - auto-detected) continuouses Names continuous vars modify fn form (null default - auto detected) corr_cut cutoff correlation threshold drop one vars (default = 0.9) k order interactions: defaults pairwise interactions m order functions: defaults quadratic functions raw raw orthogonal bases","code":""},{"path":"/reference/polySieveM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"prepare X matrix with flexible interactions and polynomials — polySieveM","text":"data.frame base terms interactions + basis","code":""},{"path":"/reference/selDGP.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate an experimental dataset with selection bias — selDGP","title":"Simulate an experimental dataset with selection bias — selDGP","text":"Simulate experimental dataset selection bias","code":""},{"path":"/reference/selDGP.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate an experimental dataset with selection bias — selDGP","text":"","code":"selDGP(   n = 10000,   p = 10,   treatProb = 0.5,   xb = c(-1, 1),   piF = function(x) 1/2,   tauF = function(x) 1/exp(-x[3]),   y0F = function(x) 3 * pmax(x[1] + x[2], 0) + 5 * sin(x[5]) * 2 * pmax(x[7], 0.5),   selF = function(x) x[1] - 5 * x[3] + pmax(x[4], 0) )"},{"path":"/reference/selDGP.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate an experimental dataset with selection bias — selDGP","text":"n sample size (default 10000) p number covariates (default 10) xb bounds uniform random variables X (default -1, 1) piF propensity score function (fixed number indicates simple randomization - default 1/2) tauF treatment heterogeneity function (nonlinear default) y0F baseline outcome function (nonlinear default) selF selection bias function (nonlinear default). null, missingness introduced.","code":""},{"path":"/reference/selDGP.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate an experimental dataset with selection bias — selDGP","text":"list outcome (missing values corresponding s = 0), treatment, covariates, selection, true treatment effect","code":""},{"path":"/reference/summary.aipw.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary method - prints treatment effects and marginal means — summary.aipw","title":"Summary method - prints treatment effects and marginal means — summary.aipw","text":"Summary method - prints treatment effects marginal means","code":""},{"path":"/reference/summary.aipw.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary method - prints treatment effects and marginal means — summary.aipw","text":"","code":"# S3 method for aipw summary(obj)"},{"path":"/reference/summary.aipw.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary method - prints treatment effects and marginal means — summary.aipw","text":"fitted aipw object","code":""},{"path":"/reference/summary.atecal.html","id":null,"dir":"Reference","previous_headings":"","what":"summary method - prints treatment effects and marginal means — summary.atecal","title":"summary method - prints treatment effects and marginal means — summary.atecal","text":"summary method - prints treatment effects marginal means","code":""},{"path":"/reference/summary.atecal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"summary method - prints treatment effects and marginal means — summary.atecal","text":"","code":"# S3 method for atecal summary(obj)"},{"path":"/reference/summary.atecal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"summary method - prints treatment effects and marginal means — summary.atecal","text":"fitted atecal object","code":""},{"path":"/reference/summary.ategt.html","id":null,"dir":"Reference","previous_headings":"","what":"summary method - prints treatment effects and marginal means — summary.ategt","title":"summary method - prints treatment effects and marginal means — summary.ategt","text":"summary method - prints treatment effects marginal means","code":""},{"path":"/reference/summary.ategt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"summary method - prints treatment effects and marginal means — summary.ategt","text":"","code":"# S3 method for ategt summary(obj)"},{"path":"/reference/summary.ategt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"summary method - prints treatment effects and marginal means — summary.ategt","text":"fitted ategt object","code":""}]
